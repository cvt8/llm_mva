{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pdval7tUZwdZ"
   },
   "source": [
    "# Retrieval-Augmented Generation (RAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CorBhMaiZwdb"
   },
   "source": [
    "Install the Hugging Face libraries to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "id": "CPGVVOEHZwdb",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /home/nathanael/.local/lib/python3.10/site-packages (4.48.1)\n",
      "Requirement already satisfied: wikipedia in /home/nathanael/.local/lib/python3.10/site-packages (1.4.0)\n",
      "Requirement already satisfied: filelock in /home/nathanael/.local/lib/python3.10/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /home/nathanael/.local/lib/python3.10/site-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/nathanael/.local/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/nathanael/.local/lib/python3.10/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/nathanael/.local/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/nathanael/.local/lib/python3.10/site-packages (from transformers) (2023.3.23)\n",
      "Requirement already satisfied: requests in /home/nathanael/.local/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/nathanael/.local/lib/python3.10/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/nathanael/.local/lib/python3.10/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/nathanael/.local/lib/python3.10/site-packages (from transformers) (4.67.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/nathanael/.local/lib/python3.10/site-packages (from wikipedia) (4.12.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/nathanael/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/nathanael/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/nathanael/.local/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/nathanael/.local/lib/python3.10/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/nathanael/.local/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/lib/python3/dist-packages (from beautifulsoup4->wikipedia) (2.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import wikipedia\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_wikipedia_pages(page_titles):\n",
    "    \"\"\"\n",
    "    Extracts Wikipedia pages and stores them in a dictionary.\n",
    "\n",
    "    Args:\n",
    "        page_titles: A list of Wikipedia page titles to extract.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the text of each Wikipedia page.\n",
    "    \"\"\"\n",
    "\n",
    "    page_data = {}\n",
    "    for title in page_titles:\n",
    "        try:\n",
    "            page = wikipedia.page(title)\n",
    "            content = page.content.strip()\n",
    "            content = content.replace(\"\\n\", \"\")\n",
    "            page_data[page.title] = content\n",
    "        except wikipedia.exceptions.PageError:\n",
    "            print(f\"Page '{title}' not found.\")\n",
    "        except wikipedia.exceptions.DisambiguationError as e:\n",
    "            print(f\"Disambiguation error for '{title}': {e.options}\")\n",
    "\n",
    "    return page_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_titles = [\n",
    "               \"Roger ApÃ©ry\",\n",
    "               \"Owen Willans Richardson\",\n",
    "               \"Otto Sackur\",\n",
    "               \"Ludvig Lorenz\",\n",
    "               \"Klaus von Klitzing\",\n",
    "               \"Henri Victor Regnault\",\n",
    "               \"Erwin Madelung\",\n",
    "              ]\n",
    "\n",
    "# Uncomment the next line to scroll through Wikipedia\n",
    "# wikipedia_data = extract_wikipedia_pages(page_titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the dictionary using `json.dump()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('wikipedia_data.json', 'w') as f:\n",
    "#     json.dump(wikipedia_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dictionary using `json.load()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('wikipedia_data.json', 'r') as f:\n",
    "    wikipedia_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3107\n",
      "3455\n",
      "1683\n",
      "1873\n",
      "1762\n",
      "3431\n",
      "1487\n"
     ]
    }
   ],
   "source": [
    "for doc in wikipedia_data:\n",
    "    print(len(wikipedia_data[doc]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load just the tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "model_max_length = tokenizer.model_max_length\n",
    "model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] hello [SEP] how are you? [SEP]'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_text = tokenizer.encode([\"hello\", \"how are you?\"])\n",
    "tokenizer.decode(encoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_splitting(text, chunk_length = 300, chunk_overlap = 100):\n",
    "    pass\n",
    "\n",
    "text_splitting(\"\".join([str(x) for x in range(20)]), 5, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text_splitting_paragraph' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m wikipedia_data_splits \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m wikipedia_data\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m----> 4\u001b[0m     wikipedia_data_splits[doc] \u001b[38;5;241m=\u001b[39m \u001b[43mtext_splitting_paragraph\u001b[49m(wikipedia_data[doc])\n\u001b[1;32m      6\u001b[0m first_key \u001b[38;5;241m=\u001b[39m page_titles[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      7\u001b[0m wikipedia_data_splits[first_key][:\u001b[38;5;241m3\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'text_splitting_paragraph' is not defined"
     ]
    }
   ],
   "source": [
    "wikipedia_data_splits = {}\n",
    "\n",
    "for doc in wikipedia_data.keys():\n",
    "    wikipedia_data_splits[doc] = text_splitting_paragraph(wikipedia_data[doc])\n",
    "\n",
    "first_key = page_titles[0]\n",
    "wikipedia_data_splits[first_key][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "min() iterable argument is empty",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m min_doc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mmin\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mwikipedia_data_splits\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mwikipedia_data_splits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m max_doc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mlen\u001b[39m(wikipedia_data_splits[doc]) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m wikipedia_data_splits)\n\u001b[1;32m      3\u001b[0m av_doc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mlen\u001b[39m(wikipedia_data_splits[doc]) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m wikipedia_data_splits) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(wikipedia_data_splits)\n",
      "\u001b[0;31mValueError\u001b[0m: min() iterable argument is empty"
     ]
    }
   ],
   "source": [
    "min_doc = min(len(wikipedia_data_splits[doc]) for doc in wikipedia_data_splits)\n",
    "max_doc = max(len(wikipedia_data_splits[doc]) for doc in wikipedia_data_splits)\n",
    "av_doc = sum(len(wikipedia_data_splits[doc]) for doc in wikipedia_data_splits) / len(wikipedia_data_splits)\n",
    "\n",
    "min_doc,max_doc,av_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load the embedder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 384, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 384)\n",
       "    (token_type_embeddings): Embedding(2, 384)\n",
       "    (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-5): 6 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(\"Hello, world!\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "output_dim = outputs.last_hidden_state.size(2)\n",
    "output_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embedder needs to know whether the document is a document or a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(chunk_list, doc_type=\"document\"):\n",
    "    encoded_docs = tokenizer([\"search_{}: {}\".format(doc_type, chunk) for chunk in chunk_list],\n",
    "                                 padding = True,\n",
    "                                 return_tensors=\"pt\")\n",
    "    output = model(**encoded_docs) # (batch, input_length, output_dim)\n",
    "    token_embeddings = output.last_hidden_state\n",
    "    output_embeddings = torch.sum(token_embeddings, 1)\n",
    "    output_embeddings = F.normalize(output_embeddings, p=2, dim=1)\n",
    "    return output_embeddings # (batch, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 384])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed([\"hello\", \"another document\", \"and another one\"]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1**: chunks may lack context. The ideal of `contextual embeddings` is to ask an LLM to write some context about the chunk (given the full document and the chunk), and to embed the chunk together with the context.\n",
    "Implement this idea here (choose a simple enough model and the appropriate task!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m     chunk_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m chunk_list, vectorial_database \u001b[38;5;241m=\u001b[39m populate_database(wikipedia_data_splits)\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "def populate_database(dic_splits, batch_size = 1):\n",
    "    n_chunks = sum([len(dic_splits[doc]) for doc in dic_splits])\n",
    "    vectorial_database = torch.zeros([n_chunks, output_dim], requires_grad = False).to(device)\n",
    "    chunk_list = []\n",
    "    pass\n",
    "\n",
    "chunk_list, vectorial_database = populate_database(wikipedia_data_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the vectorial database using `torch.save()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vectorial_database' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(\u001b[43mvectorial_database\u001b[49m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvectorial_database.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchunk_list.json\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      4\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(chunk_list, f, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vectorial_database' is not defined"
     ]
    }
   ],
   "source": [
    "torch.save(vectorial_database, 'vectorial_database.pth')\n",
    "\n",
    "with open('chunk_list.json', 'w') as f:\n",
    "    json.dump(chunk_list, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the database using `torch.load()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorial_database = torch.load('vectorial_database.pth')\n",
    "vectorial_database.to(device)\n",
    "vectorial_database.requires_grad_(False)\n",
    "\n",
    "with open('chunk_list.json', 'r') as f:\n",
    "    chunk_list = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(67, torch.Size([67, 384]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunk_list), vectorial_database.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0557,  0.0384, -0.0307, -0.0143, -0.0188], device='cuda:0') Roger ApÃ©ry (French: [apeÊi]; 14 November 1916, Ro\n",
      "tensor([ 0.0187,  0.0690, -0.0273, -0.0288,  0.0369], device='cuda:0') == Biography ==ApÃ©ry was born in Rouen in 1916 to \n",
      "tensor([ 0.0271,  0.0228, -0.0974,  0.0471, -0.0042], device='cuda:0')   His studies were interrupted at the start of Wor\n",
      "tensor([-0.0875,  0.0111, -0.0129, -0.0130, -0.1274], device='cuda:0')  He wrote his doctoral thesis in algebraic geometr\n",
      "tensor([-0.0636,  0.0460, -0.0357,  0.1095, -0.0656], device='cuda:0')  In 1949 he was appointed Professor at the Univers\n",
      "tensor([-0.1005,  0.0247, -0.0105,  0.1190,  0.0525], device='cuda:0')  An indication of the difficulty is that the corre\n",
      "tensor([-0.1744,  0.0473,  0.0322,  0.0616,  0.0066], device='cuda:0')  Nevertheless, many mathematicians have since work\n",
      "tensor([ 0.0378,  0.0296, -0.0356,  0.0142,  0.0272], device='cuda:0') ApÃ©ry was active in politics and for a few years i\n",
      "tensor([-0.0365,  0.0051,  0.0233,  0.0197, -0.0063], device='cuda:0') == Personal life ==ApÃ©ry married in 1947 and had t\n",
      "tensor([-0.0262,  0.0947, -0.0276, -0.0283, -0.0226], device='cuda:0')  He was buried next to his parents at the PÃ¨re Lac\n",
      "tensor([-0.1003,  0.0270,  0.0461,  0.0998, -0.0332], device='cuda:0')                     1        +                    \n",
      "tensor([-0.0699,  0.0599, -0.0151,  0.0524,  0.0431], device='cuda:0')  \"Roger ApÃ©ry, 1916-1994: A Radical Mathematician\"\n",
      "tensor([-0.1280,  0.0374, -0.0519,  0.0955,  0.0142], device='cuda:0') Sir Owen Willans Richardson (26 April 1879 â 15 Fe\n",
      "tensor([-0.0384, -0.0073, -0.0479,  0.0775, -0.0003], device='cuda:0') == Biography ==Richardson was born in Dewsbury, Yo\n",
      "tensor([-0.0943,  0.0029, -0.0153,  0.0672,  0.0005], device='cuda:0')  He then got a DSc from University College London \n",
      "tensor([-0.1631,  0.0333, -0.0570,  0.1261, -0.0278], device='cuda:0')  In 1901, he demonstrated that the current from a \n",
      "tensor([-0.0614,  0.0749, -0.0084,  0.0876, -0.0698], device='cuda:0')  This became known as Richardson's law: \"If then t\n",
      "tensor([-0.0752, -0.0443, -0.0122,  0.0639, -0.0128], device='cuda:0') \"Richardson was professor at Princeton University \n",
      "tensor([-0.0138,  0.0190, -0.0107,  0.0661,  0.0049], device='cuda:0')   In 1927, he was one of the participants of the f\n",
      "tensor([-0.0982,  0.0068, -0.0371,  0.0788,  0.0094], device='cuda:0') He also researched the photoelectric effect, the g\n"
     ]
    }
   ],
   "source": [
    "for i, embedding_vector in enumerate(vectorial_database[:20]):\n",
    "    print(embedding_vector[:5], chunk_list[i][:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(query_embeddings, doc_embeddings):\n",
    "    return query_embeddings @ doc_embeddings.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6542],\n",
      "        [0.4483]])\n"
     ]
    }
   ],
   "source": [
    "query_embeddings = embed([\n",
    "    \"What is TSNE?\",\n",
    "    \"Who is Laurens van der Maaten?\",\n",
    "], \"query\")\n",
    "\n",
    "doc_embeddings = embed([\n",
    "    \"TSNE is a dimensionality reduction algorithm created by Laurens van Der Maaten\",\n",
    "], \"document\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(similarity(query_embeddings, doc_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(query, \n",
    "             vectorial_database = vectorial_database, \n",
    "             chunk_list = chunk_list, \n",
    "             topk = 5,\n",
    "             verbose = False):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37.2 ns Â± 0.335 ns per loop (mean Â± std. dev. of 7 runs, 10,000,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "retrieve(\"What did Erwin Madelung study?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2**: The similarity measure is based on embeddings. A completely different approach is `lexical matching`, meaning by matching keywords from the query to the documents. It is based on `TF-IDF (Term Frequency-Inverse Document Frequency)`, as follows:\n",
    "* Compute TF-IDF for each chunk\n",
    "* BM25 returns the 25 most relevant chunks based on their TF-IDF match to the query\n",
    "\n",
    "Implement this approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3**:\n",
    "A `reranker` is (yet another) LLM which looks at the query and some chunks and ranks them by relevance. \n",
    "Implement this approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For information: Claude combines BM25 with similarity measures as follows:\n",
    "* Use BM25 to retrieve 25 chunks\n",
    "* independently, use similarity measure on embeddings to retrieve 25 chunks\n",
    "* Use a reranker to combine and deduplicate the obtained 50 chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative retrieval: SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import svm\n",
    "\n",
    "def retrieve_SVM(query, \n",
    "             vectorial_database = vectorial_database, \n",
    "             chunk_list = chunk_list, \n",
    "             topk = 5):\n",
    "    query_embedding = embed([query], \"query\")\n",
    "    x = np.concatenate([query_embedding.detach().numpy(), vectorial_database.detach().numpy()])\n",
    "    y = np.zeros(vectorial_database.size(0) + 1)\n",
    "    y[0] = 1 # we have a single positive example\n",
    "\n",
    "    clf = svm.LinearSVC(class_weight='balanced', verbose=False, max_iter=10000, tol=1e-6, C=0.1, dual=\"auto\")\n",
    "    clf.fit(x, y)\n",
    "    similarities = clf.decision_function(x)\n",
    "    sorted_ix = np.argsort(-similarities)\n",
    "    for k in sorted_ix[1:topk+1]:\n",
    "        print(f\"Score: {similarities[k]:.4f}\\nText:\\n\", chunk_list[k-1], \"\\n\")\n",
    "    return \"\\n\".join([chunk_list[k-1] for k in sorted_ix[1:topk+1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mretrieve_SVM\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat did Erwin Madelung study?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[26], line 9\u001b[0m, in \u001b[0;36mretrieve_SVM\u001b[0;34m(query, vectorial_database, chunk_list, topk)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mretrieve_SVM\u001b[39m(query, \n\u001b[1;32m      5\u001b[0m              vectorial_database \u001b[38;5;241m=\u001b[39m vectorial_database, \n\u001b[1;32m      6\u001b[0m              chunk_list \u001b[38;5;241m=\u001b[39m chunk_list, \n\u001b[1;32m      7\u001b[0m              topk \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m      8\u001b[0m     query_embedding \u001b[38;5;241m=\u001b[39m embed([query], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m     x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([query_embedding\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy(), \u001b[43mvectorial_database\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m])\n\u001b[1;32m     10\u001b[0m     y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(vectorial_database\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     11\u001b[0m     y[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;66;03m# we have a single positive example\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "retrieve_SVM(\"What did Erwin Madelung study?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model does **extractive** question answering, meaning it can only points to the answer in the provided context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, pipeline\n",
    "\n",
    "model_name = \"deepset/tinyroberta-squad2\"\n",
    "\n",
    "QA = pipeline('question-answering', model=model_name, tokenizer=model_name, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query(prompt):\n",
    "    topk_chunks = retrieve(prompt)\n",
    "#     topk_chunks = retrieve_SVM(prompt)\n",
    "    return QA(question=prompt, context=topk_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Arguments can't be understood",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat did Erwin Madelung study?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[29], line 4\u001b[0m, in \u001b[0;36mquery\u001b[0;34m(prompt)\u001b[0m\n\u001b[1;32m      2\u001b[0m     topk_chunks \u001b[38;5;241m=\u001b[39m retrieve(prompt)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#     topk_chunks = retrieve_SVM(prompt)\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mQA\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtopk_chunks\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_env/lib/python3.13/site-packages/transformers/pipelines/question_answering.py:396\u001b[0m, in \u001b[0;36mQuestionAnsweringPipeline.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args:\n\u001b[1;32m    391\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    392\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing a list of SQuAD examples to the pipeline is deprecated and will be removed in v5. Inputs should be passed using the `question` and `context` keyword arguments instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    393\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    394\u001b[0m     )\n\u001b[0;32m--> 396\u001b[0m examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_args_parser\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(examples, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(examples) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(examples[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_env/lib/python3.13/site-packages/transformers/pipelines/question_answering.py:208\u001b[0m, in \u001b[0;36mQuestionAnsweringArgumentHandler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    206\u001b[0m         inputs \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m\"\u001b[39m: kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m\"\u001b[39m]}]\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 208\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArguments can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be understood\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown arguments \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwargs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Arguments can't be understood"
     ]
    }
   ],
   "source": [
    "query(\"What did Erwin Madelung study?\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Training a new tokenizer from an old one",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
