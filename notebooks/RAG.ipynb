{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pdval7tUZwdZ"
   },
   "source": [
    "# Retrieval-Augmented Generation (RAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CorBhMaiZwdb"
   },
   "source": [
    "Install the Hugging Face libraries to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "CPGVVOEHZwdb",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/adminialab/miniconda3/envs/llm_env/lib/python3.13/site-packages (4.48.3)\n",
      "Requirement already satisfied: wikipedia in /home/adminialab/miniconda3/envs/llm_env/lib/python3.13/site-packages (1.4.0)\n",
      "Requirement already satisfied: filelock in /home/adminialab/miniconda3/envs/llm_env/lib/python3.13/site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /home/adminialab/miniconda3/envs/llm_env/lib/python3.13/site-packages (from transformers) (0.28.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/adminialab/miniconda3/envs/llm_env/lib/python3.13/site-packages (from transformers) (2.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/adminialab/miniconda3/envs/llm_env/lib/python3.13/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/adminialab/miniconda3/envs/llm_env/lib/python3.13/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/adminialab/miniconda3/envs/llm_env/lib/python3.13/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/adminialab/miniconda3/envs/llm_env/lib/python3.13/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/adminialab/miniconda3/envs/llm_env/lib/python3.13/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/adminialab/miniconda3/envs/llm_env/lib/python3.13/site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/adminialab/miniconda3/envs/llm_env/lib/python3.13/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/adminialab/miniconda3/envs/llm_env/lib/python3.13/site-packages (from wikipedia) (4.13.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/adminialab/miniconda3/envs/llm_env/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/adminialab/miniconda3/envs/llm_env/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/adminialab/miniconda3/envs/llm_env/lib/python3.13/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/adminialab/miniconda3/envs/llm_env/lib/python3.13/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/adminialab/miniconda3/envs/llm_env/lib/python3.13/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/adminialab/miniconda3/envs/llm_env/lib/python3.13/site-packages (from requests->transformers) (2024.12.14)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/adminialab/miniconda3/envs/llm_env/lib/python3.13/site-packages (from beautifulsoup4->wikipedia) (2.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import wikipedia\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_wikipedia_pages(page_titles):\n",
    "    \"\"\"\n",
    "    Extracts Wikipedia pages and stores them in a dictionary.\n",
    "\n",
    "    Args:\n",
    "        page_titles: A list of Wikipedia page titles to extract.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the text of each Wikipedia page.\n",
    "    \"\"\"\n",
    "\n",
    "    page_data = {}\n",
    "    for title in page_titles:\n",
    "        try:\n",
    "            page = wikipedia.page(title)\n",
    "            content = page.content.strip()\n",
    "            content = content.replace(\"\\n\", \"\")\n",
    "            page_data[page.title] = content\n",
    "        except wikipedia.exceptions.PageError:\n",
    "            print(f\"Page '{title}' not found.\")\n",
    "        except wikipedia.exceptions.DisambiguationError as e:\n",
    "            print(f\"Disambiguation error for '{title}': {e.options}\")\n",
    "\n",
    "    return page_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_titles = [\n",
    "               \"Roger Apéry\",\n",
    "               \"Owen Willans Richardson\",\n",
    "               \"Otto Sackur\",\n",
    "               \"Ludvig Lorenz\",\n",
    "               \"Klaus von Klitzing\",\n",
    "               \"Henri Victor Regnault\",\n",
    "               \"Erwin Madelung\",\n",
    "              ]\n",
    "\n",
    "# Uncomment the next line to scroll through Wikipedia\n",
    "# wikipedia_data = extract_wikipedia_pages(page_titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the dictionary using `json.dump()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('wikipedia_data.json', 'w') as f:\n",
    "#     json.dump(wikipedia_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dictionary using `json.load()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('wikipedia_data.json', 'r') as f:\n",
    "    wikipedia_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3107\n",
      "3455\n",
      "1683\n",
      "1873\n",
      "1762\n",
      "3431\n",
      "1487\n"
     ]
    }
   ],
   "source": [
    "for doc in wikipedia_data:\n",
    "    print(len(wikipedia_data[doc]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load just the tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdcb20d8a8af4b4b8e64804a95f9b1e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9ff177750d046fdac3fe5db140231c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3af0895ec15d4ab5856435fe38fa538f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d37d4b3ca653421bb5fd5b3cac60ecb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "model_max_length = tokenizer.model_max_length\n",
    "model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] hello [SEP] how are you? [SEP]'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_text = tokenizer.encode([\"hello\", \"how are you?\"])\n",
    "tokenizer.decode(encoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['01234',\n",
       " '34567',\n",
       " '67891',\n",
       " '91011',\n",
       " '11121',\n",
       " '21314',\n",
       " '14151',\n",
       " '51617',\n",
       " '17181',\n",
       " '819']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def text_splitting(text, chunk_length = 300, chunk_overlap = 100):\n",
    "    out = []\n",
    "    for i in range(0,len(text), chunk_length - chunk_overlap):\n",
    "        out.append(text[i: i + chunk_length])\n",
    "    return out\n",
    "\n",
    "text_splitting(\"\".join([str(x) for x in range(20)]), 5, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_splitting_paragraph(text, chunk_length = 300):\n",
    "    out = []\n",
    "    paragraph_list = text.split(\".\")\n",
    "    current_text = \"\"\n",
    "    length = 0\n",
    "    for par in paragraph_list:\n",
    "        if length != 0 and length + len(par) < chunk_length:\n",
    "            current_text += \". \" + par\n",
    "            length += len(par)\n",
    "        else:\n",
    "            if len(current_text) != 0:\n",
    "                out.append(current_text + \".\")\n",
    "            current_text = par\n",
    "            length = len(par)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Roger Apéry (French: [apeʁi]; 14 November 1916, Rouen – 18 December 1994, Caen) was a French mathematician most remembered for Apéry's theorem, which states that ζ(3) is an irrational number.  Here, ζ(s) denotes the Riemann zeta function.\",\n",
       " '== Biography ==Apéry was born in Rouen in 1916 to a French mother and Greek father.  His childhood was spent in Lille until 1926, when the family moved to Paris, where he studied at the Lycée Ledru-Rollin and the Lycée Louis-le-Grand.   He was admitted  at the École normale supérieure in 1935.',\n",
       " '  His studies were interrupted at the start of World War II; he was mobilized in September 1939, taken prisoner of war in June 1940, repatriated with pleurisy in June 1941, and hospitalized until August 1941.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikipedia_data_splits = {}\n",
    "\n",
    "for doc in wikipedia_data.keys():\n",
    "    wikipedia_data_splits[doc] = text_splitting_paragraph(wikipedia_data[doc])\n",
    "\n",
    "first_key = page_titles[0]\n",
    "wikipedia_data_splits[first_key][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 14, 9.571428571428571)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_doc = min(len(wikipedia_data_splits[doc]) for doc in wikipedia_data_splits)\n",
    "max_doc = max(len(wikipedia_data_splits[doc]) for doc in wikipedia_data_splits)\n",
    "av_doc = sum(len(wikipedia_data_splits[doc]) for doc in wikipedia_data_splits) / len(wikipedia_data_splits)\n",
    "\n",
    "min_doc,max_doc,av_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load the embedder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0ec081df268421cade21c0c6daef9f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a30af6f5aa79477ab2cf16be72cf988a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 384, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 384)\n",
       "    (token_type_embeddings): Embedding(2, 384)\n",
       "    (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-5): 6 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(\"Hello, world!\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "output_dim = outputs.last_hidden_state.size(2)\n",
    "output_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embedder needs to know whether the document is a document or a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(chunk_list, doc_type=\"document\"):\n",
    "    encoded_docs = tokenizer([\"search_{}: {}\".format(doc_type, chunk) for chunk in chunk_list],\n",
    "                                 padding = True,\n",
    "                                 return_tensors=\"pt\")\n",
    "    output = model(**encoded_docs) # (batch, input_length, output_dim)\n",
    "    token_embeddings = output.last_hidden_state\n",
    "    output_embeddings = torch.sum(token_embeddings, 1)\n",
    "    output_embeddings = F.normalize(output_embeddings, p=2, dim=1)\n",
    "    return output_embeddings # (batch, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 384])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed([\"hello\", \"another document\", \"and another one\"]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roger Apéry 12\n",
      "Owen Richardson 14\n",
      "Otto Sackur 6\n",
      "Ludvig Lorenz 8\n",
      "Klaus von Klitzing 7\n",
      "Henri Victor Regnault 14\n",
      "Erwin Madelung 6\n"
     ]
    }
   ],
   "source": [
    "def populate_database(dic_splits, batch_size = 1):\n",
    "    n_chunks = sum([len(dic_splits[doc]) for doc in dic_splits])\n",
    "    vectorial_database = torch.zeros([n_chunks, output_dim], requires_grad = False).to(device)\n",
    "    chunk_list = []\n",
    "    n = 0\n",
    "    for doc in dic_splits.keys():\n",
    "        chunk_list_doc = dic_splits[doc]\n",
    "        print(doc, len(chunk_list_doc))\n",
    "        for i in range(0, len(chunk_list_doc), batch_size):\n",
    "            batch = chunk_list_doc[i:i+batch_size]\n",
    "            chunk_list += batch\n",
    "            embeddings = embed(batch, doc_type = \"document\")\n",
    "            vectorial_database[n:n+len(batch)] = embeddings\n",
    "            n += len(batch)\n",
    "    return chunk_list, vectorial_database\n",
    "\n",
    "# Uncomment this to populate the database\n",
    "chunk_list, vectorial_database = populate_database(wikipedia_data_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the vectorial database using `torch.save()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(vectorial_database, 'vectorial_database.pth')\n",
    "\n",
    "with open('chunk_list.json', 'w') as f:\n",
    "    json.dump(chunk_list, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the database using `torch.load()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorial_database = torch.load('vectorial_database.pth')\n",
    "vectorial_database.to(device)\n",
    "vectorial_database.requires_grad_(False)\n",
    "\n",
    "with open('chunk_list.json', 'r') as f:\n",
    "    chunk_list = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(67, torch.Size([67, 384]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunk_list), vectorial_database.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0557,  0.0384, -0.0307, -0.0143, -0.0188], device='cuda:0') Roger Apéry (French: [apeʁi]; 14 November 1916, Ro\n",
      "tensor([ 0.0187,  0.0690, -0.0273, -0.0288,  0.0369], device='cuda:0') == Biography ==Apéry was born in Rouen in 1916 to \n",
      "tensor([ 0.0271,  0.0228, -0.0974,  0.0471, -0.0042], device='cuda:0')   His studies were interrupted at the start of Wor\n",
      "tensor([-0.0875,  0.0111, -0.0129, -0.0130, -0.1274], device='cuda:0')  He wrote his doctoral thesis in algebraic geometr\n",
      "tensor([-0.0636,  0.0460, -0.0357,  0.1095, -0.0656], device='cuda:0')  In 1949 he was appointed Professor at the Univers\n",
      "tensor([-0.1005,  0.0247, -0.0105,  0.1190,  0.0525], device='cuda:0')  An indication of the difficulty is that the corre\n",
      "tensor([-0.1744,  0.0473,  0.0322,  0.0616,  0.0066], device='cuda:0')  Nevertheless, many mathematicians have since work\n",
      "tensor([ 0.0378,  0.0296, -0.0356,  0.0142,  0.0272], device='cuda:0') Apéry was active in politics and for a few years i\n",
      "tensor([-0.0365,  0.0051,  0.0233,  0.0197, -0.0063], device='cuda:0') == Personal life ==Apéry married in 1947 and had t\n",
      "tensor([-0.0262,  0.0947, -0.0276, -0.0283, -0.0226], device='cuda:0')  He was buried next to his parents at the Père Lac\n",
      "tensor([-0.1003,  0.0270,  0.0461,  0.0998, -0.0332], device='cuda:0')                     1        +                    \n",
      "tensor([-0.0699,  0.0599, -0.0151,  0.0524,  0.0431], device='cuda:0')  \"Roger Apéry, 1916-1994: A Radical Mathematician\"\n",
      "tensor([-0.1280,  0.0374, -0.0519,  0.0955,  0.0142], device='cuda:0') Sir Owen Willans Richardson (26 April 1879 – 15 Fe\n",
      "tensor([-0.0384, -0.0073, -0.0479,  0.0775, -0.0003], device='cuda:0') == Biography ==Richardson was born in Dewsbury, Yo\n",
      "tensor([-0.0943,  0.0029, -0.0153,  0.0672,  0.0005], device='cuda:0')  He then got a DSc from University College London \n",
      "tensor([-0.1631,  0.0333, -0.0570,  0.1261, -0.0278], device='cuda:0')  In 1901, he demonstrated that the current from a \n",
      "tensor([-0.0614,  0.0749, -0.0084,  0.0876, -0.0698], device='cuda:0')  This became known as Richardson's law: \"If then t\n",
      "tensor([-0.0752, -0.0443, -0.0122,  0.0639, -0.0128], device='cuda:0') \"Richardson was professor at Princeton University \n",
      "tensor([-0.0138,  0.0190, -0.0107,  0.0661,  0.0049], device='cuda:0')   In 1927, he was one of the participants of the f\n",
      "tensor([-0.0982,  0.0068, -0.0371,  0.0788,  0.0094], device='cuda:0') He also researched the photoelectric effect, the g\n"
     ]
    }
   ],
   "source": [
    "for i, embedding_vector in enumerate(vectorial_database[:20]):\n",
    "    print(embedding_vector[:5], chunk_list[i][:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(query_embeddings, doc_embeddings):\n",
    "    return query_embeddings @ doc_embeddings.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6542],\n",
      "        [0.4483]])\n"
     ]
    }
   ],
   "source": [
    "query_embeddings = embed([\n",
    "    \"What is TSNE?\",\n",
    "    \"Who is Laurens van der Maaten?\",\n",
    "], \"query\")\n",
    "\n",
    "doc_embeddings = embed([\n",
    "    \"TSNE is a dimensionality reduction algorithm created by Laurens van Der Maaten\",\n",
    "], \"document\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(similarity(query_embeddings, doc_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(query, \n",
    "             vectorial_database = vectorial_database, \n",
    "             chunk_list = chunk_list, \n",
    "             topk = 5,\n",
    "             verbose = False):\n",
    "    query_embedding = embed([query], \"query\")\n",
    "    query_embedding.to(device)\n",
    "    similarity_scores = similarity(query_embedding, vectorial_database)\n",
    "    topk = torch.topk(similarity_scores, k = topk)\n",
    "\n",
    "    if verbose:\n",
    "        for score, idx in zip(topk.values[0], topk.indices[0]):\n",
    "            print(f\"Score: {score:.4f}\\nText:\\n\", chunk_list[idx], \"\\n\")\n",
    "    return \"\\n\".join([chunk_list[i] for i in topk.indices[0].tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat2 in method wrapper_CUDA_mm)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtimeit\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mretrieve(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat did Erwin Madelung study?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_env/lib/python3.13/site-packages/IPython/core/interactiveshell.py:2543\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2541\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[1;32m   2542\u001b[0m     args \u001b[38;5;241m=\u001b[39m (magic_arg_s, cell)\n\u001b[0;32m-> 2543\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2545\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2546\u001b[0m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2547\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2548\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_env/lib/python3.13/site-packages/IPython/core/magics/execution.py:1209\u001b[0m, in \u001b[0;36mExecutionMagics.timeit\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1207\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m   1208\u001b[0m     number \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m index\n\u001b[0;32m-> 1209\u001b[0m     time_number \u001b[38;5;241m=\u001b[39m \u001b[43mtimer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnumber\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1210\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m time_number \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.2\u001b[39m:\n\u001b[1;32m   1211\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_env/lib/python3.13/site-packages/IPython/core/magics/execution.py:173\u001b[0m, in \u001b[0;36mTimer.timeit\u001b[0;34m(self, number)\u001b[0m\n\u001b[1;32m    171\u001b[0m gc\u001b[38;5;241m.\u001b[39mdisable()\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 173\u001b[0m     timing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gcold:\n",
      "File \u001b[0;32m<magic-timeit>:1\u001b[0m, in \u001b[0;36minner\u001b[0;34m(_it, _timer)\u001b[0m\n",
      "Cell \u001b[0;32mIn[26], line 8\u001b[0m, in \u001b[0;36mretrieve\u001b[0;34m(query, vectorial_database, chunk_list, topk, verbose)\u001b[0m\n\u001b[1;32m      6\u001b[0m query_embedding \u001b[38;5;241m=\u001b[39m embed([query], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m query_embedding\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 8\u001b[0m similarity_scores \u001b[38;5;241m=\u001b[39m \u001b[43msimilarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_embedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvectorial_database\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m topk \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtopk(similarity_scores, k \u001b[38;5;241m=\u001b[39m topk)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n",
      "Cell \u001b[0;32mIn[24], line 2\u001b[0m, in \u001b[0;36msimilarity\u001b[0;34m(query_embeddings, doc_embeddings)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msimilarity\u001b[39m(query_embeddings, doc_embeddings):\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mquery_embeddings\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdoc_embeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat2 in method wrapper_CUDA_mm)"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "retrieve(\"What did Erwin Madelung study?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative retrieval: SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import svm\n",
    "\n",
    "def retrieve_SVM(query, \n",
    "             vectorial_database = vectorial_database, \n",
    "             chunk_list = chunk_list, \n",
    "             topk = 5):\n",
    "    query_embedding = embed([query], \"query\")\n",
    "    x = np.concatenate([query_embedding.detach().numpy(), vectorial_database.detach().numpy()])\n",
    "    y = np.zeros(vectorial_database.size(0) + 1)\n",
    "    y[0] = 1 # we have a single positive example\n",
    "\n",
    "    clf = svm.LinearSVC(class_weight='balanced', verbose=False, max_iter=10000, tol=1e-6, C=0.1, dual=\"auto\")\n",
    "    clf.fit(x, y)\n",
    "    similarities = clf.decision_function(x)\n",
    "    sorted_ix = np.argsort(-similarities)\n",
    "    for k in sorted_ix[1:topk+1]:\n",
    "        print(f\"Score: {similarities[k]:.4f}\\nText:\\n\", chunk_list[k-1], \"\\n\")\n",
    "    return \"\\n\".join([chunk_list[k-1] for k in sorted_ix[1:topk+1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mretrieve_SVM\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat did Erwin Madelung study?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[28], line 9\u001b[0m, in \u001b[0;36mretrieve_SVM\u001b[0;34m(query, vectorial_database, chunk_list, topk)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mretrieve_SVM\u001b[39m(query, \n\u001b[1;32m      5\u001b[0m              vectorial_database \u001b[38;5;241m=\u001b[39m vectorial_database, \n\u001b[1;32m      6\u001b[0m              chunk_list \u001b[38;5;241m=\u001b[39m chunk_list, \n\u001b[1;32m      7\u001b[0m              topk \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m      8\u001b[0m     query_embedding \u001b[38;5;241m=\u001b[39m embed([query], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m     x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([query_embedding\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy(), \u001b[43mvectorial_database\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m])\n\u001b[1;32m     10\u001b[0m     y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(vectorial_database\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     11\u001b[0m     y[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;66;03m# we have a single positive example\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "retrieve_SVM(\"What did Erwin Madelung study?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model does **extractive** question answering, meaning it can only points to the answer in the provided context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adafdf94edd2437d8b446d3db131ce0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/835 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab70aedf07224fd78df02b233e4165df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/326M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2e364cfe8734613b6a9bef7eb56517b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/383 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edd4b49e55bf476ca80162d423105aee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b81972bb1f2d4ef98ef508b2d2a069c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a05049594014d3894c059e523bd275d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e4ce94b247b4ab396c25900aa7dece7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, pipeline\n",
    "\n",
    "model_name = \"deepset/tinyroberta-squad2\"\n",
    "\n",
    "QA = pipeline('question-answering', model=model_name, tokenizer=model_name, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query(prompt):\n",
    "    topk_chunks = retrieve(prompt)\n",
    "#     topk_chunks = retrieve_SVM(prompt)\n",
    "    return QA(question=prompt, context=topk_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat2 in method wrapper_CUDA_mm)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat did Erwin Madelung study?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[31], line 2\u001b[0m, in \u001b[0;36mquery\u001b[0;34m(prompt)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mquery\u001b[39m(prompt):\n\u001b[0;32m----> 2\u001b[0m     topk_chunks \u001b[38;5;241m=\u001b[39m \u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#     topk_chunks = retrieve_SVM(prompt)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m QA(question\u001b[38;5;241m=\u001b[39mprompt, context\u001b[38;5;241m=\u001b[39mtopk_chunks)\n",
      "Cell \u001b[0;32mIn[26], line 8\u001b[0m, in \u001b[0;36mretrieve\u001b[0;34m(query, vectorial_database, chunk_list, topk, verbose)\u001b[0m\n\u001b[1;32m      6\u001b[0m query_embedding \u001b[38;5;241m=\u001b[39m embed([query], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m query_embedding\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 8\u001b[0m similarity_scores \u001b[38;5;241m=\u001b[39m \u001b[43msimilarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_embedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvectorial_database\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m topk \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtopk(similarity_scores, k \u001b[38;5;241m=\u001b[39m topk)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n",
      "Cell \u001b[0;32mIn[24], line 2\u001b[0m, in \u001b[0;36msimilarity\u001b[0;34m(query_embeddings, doc_embeddings)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msimilarity\u001b[39m(query_embeddings, doc_embeddings):\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mquery_embeddings\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdoc_embeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat2 in method wrapper_CUDA_mm)"
     ]
    }
   ],
   "source": [
    "query(\"What did Erwin Madelung study?\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Training a new tokenizer from an old one",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
